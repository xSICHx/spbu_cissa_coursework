\documentclass[a4paper, 11pt]{article}

\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\usepackage[T1]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[normalem]{ulem}  % для зачекивания текста
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{float}

\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\doublenorm}[1]{\left\lVert #1 \right\rVert}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}

\setcounter{tocdepth}{2}

\newcommand{\SSA}{\textbf{SSA}}
\newcommand{\CISSA}{\textbf{CiSSA}}
\newcommand{\TS}{\mathsf{X}}

\newtheorem{definition}{Определение} % задаём выводимое слово (для определений)
\newtheorem{theorem}{Теорема} % задаём выводимое слово (для определений)

%\usepackage{fleqn}


%##################


\date{}
\begin{document}

%\input{../Титульник/report.tex}

\tableofcontents

\newpage

\section{Введение}

Временные ряды представляют собой последовательность данных, собранных или
измеренных в хронологическом порядке. Они играют ключевую роль в анализе и прогнозировании
в различных областях, таких как экономика, финансы, климатология,
медицина. Понимание эволюции явлений во времени является критическим для выявления
тенденций, циклов и аномалий.

Сингулярный спектральный анализ ($\SSA$) — метод, целью которого является разложение оригинального ряда на сумму небольшого числа интерпретируемых компонентов, таких как медленно изменяющаяся тенденция (тренд), колебательные компоненты (сезонность) и ``структурный'' шум. Основной концепцией при изучении свойств методов $\SSA$ является ``разделимость'', которая характеризует, насколько хорошо разные компоненты могут быть отделены друг от друга. В данном исследовании
рассматривается математическая составляющая вариации алгоритма $\SSA$ --- circulant singular spectrum analysis ($\CISSA$), предложенная в статье \cite{bogalo2020}, а также сравнение базового метода и циркулярного, применение их на языке R.

\subsection{Постановка задачи}
\label{sec:tasks}
Перед началом исследования были поставлены следующие цели:
\begin{enumerate}
	\item Ознакомиться с алгоритмом $\CISSA$;
	\item Реализовать алгоритм $\CISSA$ на языке R; 
	\item Сравнить алгоритмы $\SSA$ и $\CISSA$.
\end{enumerate}

\newpage



\newpage

\section{Базовый метод SSA}
\label{sec:ssa}


Рассмотрим первоначальный метод сингулярного спектрального анализа \cite{golyandina2001analysis}.

\subsection{Алгоритм метода SSA}

Пусть $N > 2$, вещественнозначный временной ряд
$\TS = (x_1, \dots, x_{N})$ длины $N$.
Базовый алгоритм состоит $\SSA$ из четырех шагов.

\subsubsection{Вложение}
$L$ --- некоторое целое число (длина окна), $1 < L < N$. Строится $L$-траекторная матрица $\mathbf{X}$, состоящая из $K = N - L + 1$ векторов вложения:
\begin{equation}
	\label{eq:X}
	\mathbf{X} = 
	\begin{pmatrix}
		x_1 & x_2 & x_3 & \dots & x_{K} \\
		x_2 & x_3 & x_4 & \dots & x_{K+1} \\
		x_3 & x_4 & x_5 & \dots & x_{K+2} \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		x_{L} & x_{L+1} & x_{L+2} & \dots & x_{N}
	\end{pmatrix}.
\end{equation}
Полезным свойством является то, что матрица $\mathbf{X}$ имеет одинаковые элементы на диагоналях. Таким образом, $L$-траекторная матрица является ганкелевой.

\subsubsection{Сингулярное разложение (SVD)}
Результатом этого шага является сингулярное разложение (Singular Value Decomposition, $\mathbf{SVD}$)  траекторной матрицы ряда.

Пусть $\mathbf{S} = \mathbf{X}\mathbf{X}^{\mathrm{T}}$, 
$\lambda_1, \dots, \lambda_L$ --- собственные числа матрицы $\mathbf{S}$, взятые в неубывающем порядке и
$U_1, \dots, U_L$ --- ортонормированная система собственных векторов соответствующих собственным числам матрицы $\mathbf S$. 

Определим $d = \max{ \{i: \lambda_i > 0 \}}$ и 
$V_i = \mathbf{X}^{\mathrm{T}} U_i / \sqrt{\lambda_i}$.
Тогда сингулярным разложением называется представление матрицы в виде:
\begin{equation}
	\mathbf{X} = \mathbf{X}_1 + \dots + \mathbf{X}_d =
	\sum_{i = 1}^{d} \sqrt{\lambda_i} U_i V_{i}^{\mathrm{T}}\label{eq:1}.
\end{equation}

Набор $( \sqrt{\lambda_i}, U_i, V_{i}^{\mathrm{T}})$ называется $i$-й собственной тройкой разложения \eqref{eq:1}.

\subsubsection{Группировка}
На основе разложения \eqref{eq:1} производится процедура группировки, которая делит все множество индексов $\{1, \dots, d\}$ на $m$ непересекающихся подмножеств $I_1, \dots, I_d$. 

Пусть $I = \{i_1, \dots, i_p\}$, тогда $\mathbf{X}_I =
\mathbf{X_{i_1}} + \dots + \mathbf{X_{i_p}}$. Такие матрицы вычисляются для каждого $I = I_1, \dots, I_m$. 
В результате получаются матрицы $\mathbf{X_{I_1}}, \dots, \mathbf{X_{I_m}}$. Тем самым разложение \eqref{eq:1} может быть записано в сгруппированном виде:
\begin{equation*}
	\mathbf{X} = \mathbf{X}_{I_1} + \dots + \mathbf{X}_{I_m}.
\end{equation*}

\subsubsection{Диагональное усреднение}
Пусть $\mathbf{Y}$ --- матрица размерности $L \times K$. $L^* = \min(L, K), \, K^* = \max(L,K)$ Диагональное усреднение переводит матрицу $\mathbf{Y}$ в временной ряд $g_0, \dots, g_{N-1} $:

\begin{equation*}
	g_{k}=
	\begin{cases}
		\frac{1}{k+1} \sum\limits_{m=1}^{k+1} y_{m,k-m+2}^{*} &
		 \text{для } 0 \leq k < L^* - 1, \\
		
		\frac{1}{L^{*}} \sum\limits_{m=1}^{L^*} y_{m,k-m+2}^{*} &
		 \text{для } L^*-1 \leq k < K^* , \\
		
		\frac{1}{N-k} \sum\limits_{m=k-K^*+2}^{N-K^*+1} y_{m,k-m+2}^{*} &
		\text{для } K^* \leq k < N .\\
	\end{cases}
\end{equation*}
Применяя данную операцию к матрицам $\mathbf{X_{I_1}}, \dots, \mathbf{X_{I_m}}$, получаются $m$ новых рядов: $\TS_1, \dots, \TS_m$. При этом, $\TS_1 + \dots + \TS_m = \TS$.

\subsection{Свойства SSA}

\subsubsection{Точная разделимость}


Пусть временной ряд  $\TS = \TS^{(1)} + \TS^{(2)}$ и задачей является нахождение этих слагаемых. В результате базового алгоритма $\SSA$ также получаем $2$ ряда. Возникает вопрос: в каких случаях мы можем так выбрать параметр алгоритма $L$ и так сгруппировать собственные тройки, чтобы получить исходные 2 ряда без смешиваний?
При выборе длины окна L, каждый из рядов $\TS^{(1)}$, $\TS^{(2)}$, $\TS$ порождает траекторную матрицу $\mathbf{X}^{(1)}, \mathbf{X}^{(2)}, \mathbf{X}$.

\begin{definition}
	Будем говорить, что ряды $\TS^{(1)}$ и $\TS^{(2)}$ слабо L-разделимы, если пространства, порождаемые строками $\mathbf{X}^{(1)}$ и $\mathbf{X}^{(2)}$ соответственно, ортогональны. То же самое должно выполняться для столбцов \cite{golyandina2001analysis}.
\end{definition}

Если выполняется условие слабой L-разделимости, тогда существует такое сингулярное разложение траекторной матрицы $\mathbf X$ ряда $\TS$, что его можно разбить на две части, являющиеся сингулярными разложениями траекторных матриц рядов $\TS^{(1)}, \TS^{(2)}$ \cite{golyandina2001analysis}.

\begin{definition}
	Будем говорить, что ряды $\TS^{(1)}, \TS^{(2)}$ сильно L-разделимы, если они слабо L-разделимы и после процедуры $\mathbf{SVD}$ собственные числа рядов различны \cite{golyandina2001analysis}.
\end{definition}

Если выполняется условие сильной L-разделимости, тогда любое сингулярное разложение траекторной матрицы $\mathbf X$ ряда $\TS$ можно разбить на две части, являющиеся сингулярными разложениями траекторных матриц рядов $\TS^{(1)}, \TS^{(2)}$ \cite{golyandina2001analysis}.


Рассмотрим таблицу, в которой знаком + отмечены пары рядов, для которых существуют параметры функций и параметры метода $L$ и $ K = N - L +1$, при которых они разделимы (точно разделимы). Данная таблица \ref{tab:1} и условия разделимости с доказательствами взяты из книги \cite{golyandina2001analysis}.

\begin{table}[H]
	\begin{center}
		\caption{Точная разделимость}
		\label{tab:1}
		\scalebox{1}{
			\begin{tabular}{cccccc}
				\hline
				& const & cos & exp & exp cos & ak+b \\ \hline
				const   & -     & +   & -   & -       & -    \\
				cos     & +     & +   & -   & -       & -    \\
				exp     & -     & -   & -   & +       & -    \\
				exp cos & -     & -   & +   & +       & -    \\
				ak+b    & -     & -   & -   & -       & -    \\ \hline
		\end{tabular}}
	\end{center}
\end{table}

Стоит отметить, что точная разделимость для $\cos$ достигается, если $Lw \in \mathbb{N}, \, Kw \in \mathbb{N}$, где $w$ --- частота \cite{golyandina2001analysis}.

Однако, по таблице \ref{tab:1} видно, что условия точной разделимости достаточно жесткие и вряд ли выполнимы в реальных задачах. Тогда появляется такое понятие, как асимптотическая разделимость.

\subsubsection{Асимптотическая разделимость}

Для любого ряда $\TS$ длины $N$ определим
$\TS_{i,j}\,=\,(x_{i-1},\cdot\cdot\cdot,x_{j-1}),\;\;1\,\leq\,i\,\leq\,j\,<\,N.$
Пусть $\TS^{(1)}=(x_{0}^{(1)},\ldots,x_{N-1}^{(1)}),\TS^{(2)}=(x_{0}^{(2)},\ldots,x_{N-1}^{(2)}).$ Тогда определим коэффициент корреляции следующим образом:


\begin{equation*}
	\rho_{i,j}^{(M)}=\frac{\left(\TS_{i,i+M-1}^{(1)},\TS_{j,j+M-1}^{(2)}\right)}{\left|\left|\TS_{i,i+M-1}^{(1)}\right|\right|\left|\left|\TS_{j,j+M-1}^{(2)}\right|\right|}.
\end{equation*}

\begin{definition}
	Ряды $F^{(1)}, F^{(2)}$ называются $\varepsilon$-разделимыми при длине окна $L$, если
	\begin{equation*}
		\rho^{(L,K)}\ {\stackrel{\mathrm{def}}{=}}\ \mathrm{max}\left(\operatorname*{max}_{1\leq i,j\leq K}|\rho_{i,j}^{(L)}|,\operatorname*{max}_{1\leq i,j\leq L}|\rho_{i,j}^{(K)}|\right)<\varepsilon
		\text{  \cite{golyandina2001analysis}.}
	\end{equation*}
	
\end{definition}

\begin{definition}
	Если $\rho^{(\TS(N),K(N))} \rightarrow 0$ при некоторой последовательности $L = L(N) $, $N \rightarrow \infty$, то ряды $\TS^{(1)}, \TS^{(2)}$ называются асимтпотически $L(N)$-разделимыми \cite{golyandina2001analysis}.
\end{definition}

Как можно заметить по таблице \ref{tab:2}, для гораздо большего класса функций асимптотическая разделимость имеет место \cite{golyandina2001analysis}.
\begin{table}[h]
	\begin{center}
		\caption{Асимптотическая разделимость}
		\label{tab:2}
		\scalebox{1}{
			\begin{tabular}{cccccc}
				\hline
				& const & cos & exp & exp cos & ak+b \\ \hline
				const   & -     & +   & +   & +       & -    \\
				cos     & +     & +   & +   & +       & +    \\
				exp     & +     & +   & +   & +       & +    \\
				exp cos & +     & +   & +   & +       & +    \\
				ak+b    & +     & +   & +   & +       & -    \\ \hline
		\end{tabular}}
	\end{center}
\end{table}

\newpage

\section{Метод Circulant singular spectrum analysis (CiSSA)}
\label{sec:cissa}

%* Переформулировка начала секции 3 из статьи про $\CISSA$ от третьего лица (сказать зачем он создавался, т.е. для автоматического выделения частот)

В этом разделе предложена автоматизированная версия $\SSA$ на основе циркулярной матрицы \cite{bogalo2020}. Причем автоматизированная в том смысле, что компоненты ряда группируются по частотам самим алгоритмом. Сначала будет рассмотрен метод только для стационарного случая, затем доказана его применимость при использовании нестационарного ряда.

Стационарность подразумевает неизменность статистических свойств ряда во времени. Однако определим это понятие формально \cite{golyandina2001analysis}.  
\begin{definition}
		 Пусть $\TS = (x_0, x_1, \dots, x_n, \dots)$ — временной ряд. Ряд $\TS$ называется стационарным, если существует функция $R_{\TS}(k)$ ($-\infty < k < +\infty$) такая, что для любых $k, l \geq 0$  
		 \begin{equation}
			R_{\TS}^{(N)}(k, l) \overset{\mathrm{def}}{=} \frac{1}{N} \sum_{m=0}^{N-1} x_{k+m} x_{l+m} \xrightarrow{N \to \infty} R_{\TS}(k - l). \label{eq:R}	 	
		 \end{equation}
	
	Если \eqref{eq:R} выполняется, тогда $R_{\TS}$ называется ковариационной функцией стационарного ряда $\TS$.
\end{definition}

\begin{theorem}
	Пусть $R_{\TS}$ — ковариационная функция стационарного ряда $\TS$. Тогда существует конечная мера $m_{\TS}$, определенная на борелевских подмножествах $(-1/2, 1/2]$, такая, что  
	\[
	R_{\TS}(k) = \int_{(-\frac{1}{2}, \frac{1}{2}]} e^{i 2 \pi k \omega} m_{\TS}(d\omega).
	\]
	
	
	Мера $m_{\TS}$ называется спектральной мерой ряда $\TS$.
\end{theorem}
\begin{proof}
	Доказательство в источнике \cite{golyandina2001analysis}.
\end{proof}

%\begin{definition}
%	 ряд $\TS = (x_1, x_2, x_3, \dots)$ называется стационарным, если:
%	\begin{enumerate}
%		\item $\mathrm E (x_t) \equiv \mathrm{const}, \, \forall t \in 1:N$;
%		\item $\mathrm{Cov}(x_t, x_{t+h}) \equiv \mathrm{const}$ при фиксированном h.
%	\end{enumerate}
%\end{definition}

\subsection{Алгоритм метода CiSSA}
%План:
%\begin{enumerate}
%	\item Алгоритм в текстовом виде.
%	\item ??? Алгоритм на псевдокоде.
%	\item Кратко указать, почему он работает (сослаться на доказательства в статье)
%	\item Пояснить, что делать с нестационарными рядами, показать расширение ряда
%	\item Кратко упомянуть, что алгоритм был реализован на языке R, сослаться на код в GitHub.
%\end{enumerate}

Данный алгоритм состоит также из четырех основных шагов.

Зафиксируем стационарный временной ряд $\TS$ состоящий из $N$ элементов и выберем длину окна $L$.
\subsubsection{Вложение}
Такой же, как и в $\SSA$. Считаем матрицу $\mathbf{X}$, заданную в \eqref{eq:X}.

\subsubsection{Разложение}
Будем рассматривать временной ряд как выборку после эксперимента, а не как случайную величину. Соответственно, все формулы будут выборочными.

Определим автоковарицации: 
\begin{equation*}
	\hat{\gamma}_m = \frac{1}{N-m} \sum \limits_{t = 1}^{N-m}x_t x_{t+m}, \, m = 0:L-1.
\end{equation*}
На основе $\hat{\gamma}_m$ определим матрицу:
\begin{equation}
	\label{eq:tepl_mat}
	\hat{\gamma}_{L}=\left(\begin{array}{cccc}
		\hat{\gamma}_{1} & \hat{\gamma}_{2} & \ldots & \hat{\gamma}_{L} \\
		\hat{\gamma}_{2} & \hat{\gamma}_{1} & \ldots & \hat{\gamma}_{L-1} \\
		\vdots & \vdots & \vdots & \vdots \\
		\hat{\gamma}_{L} & \hat{\gamma}_{L-1} & \hdots & \hat{\gamma}_{1}
	\end{array}\right).
\end{equation}
Данная матрица $L \times L$ называется Теплицевой и используется в методе Toeplitz SSA (подробнее про данный метод можно прочитать в книге \cite{golyandina2001analysis}). На ее основе составим циркулярную матрицу для алгоритма Circulant SSA \cite{bogalo2020}:


\begin{equation}
	\label{eq:circ_mat}
	\hat{\mathrm{C}}_{L}=\left(\begin{array}{cccc}
		\hat c_{1} & \hat c_{2} & \ldots & \hat c_{L} \\
		\hat c_{2} & \hat c_{1} & \ldots & \hat c_{L-1} \\
		\vdots & \vdots & \vdots & \vdots \\
		\hat c_{L} & \hat c_{L-1} & \hdots & \hat c_{1}
	\end{array}\right),
\end{equation}
где $\hat c_m = \frac{L-m}{L}\hat{\gamma}_m + \frac{m}{L}\hat{\gamma}_{L-m}, \, m = 0:L-1$.
Собственные числа матрицы $\hat{\mathrm{C}}_{L}$, определенной в \eqref{eq:circ_mat} задаются по формуле:
\begin{equation*}
	\lambda_{L,k}=\sum_{m=0}^{L-1}\hat c_{m}\exp\left(i 2\pi m\frac{k-1}{L}\right), \, k = 1:L, \, \text{причем} \, \lambda_{L,k} = \lambda_{L,L+2-k},
\end{equation*}
а собственные вектора, связанные с $\lambda_{L, k}$ вычисляются следующим образом:
\begin{equation*}
	{U}_{k}=L^{-1/2}(u_{k,1\cdot}\cdot\cdot\cdot,u_{k,L}), \, \text{где} \, 
	u_{k,j}=\exp\left(-\mathrm{i}2\pi\d(j-1)\frac{k-1}{L}\right), \,
	\text{причем} \, U_{k} = U_{L+2-k}^*,
\end{equation*}
где $U^*$ --- комплексное сопряжение вектора $U$.


\subsubsection{Элементарное разложение}

Для каждой частоты $w_k = \frac{k-1}{L}$, $k = 2:\lfloor \frac{L+1}{2} \rfloor$, есть два собственных вектора: $U_k$ и $U_{L+2-k}$. За частоту $w_0$ отвечает один собственный вектор --- $U_0$. Если же $L$ --- четное, то частоте $w_{\frac{L}{2} + 1}$ будет соответствовать один вектор $U_{\frac{L}{2}+1}$.

Следовательно, индексы группируются следующим образом:
\begin{equation*}
	B_1 = \{1\}; \, B_k = \{k, L+2-k\}, \,  \text{для } k = 2:\lfloor \frac{L+1}{2}\rfloor; \, 
	B_{\frac{L}{2} + 1} = \left\{ \frac{L}{2} + 1 \right\}, \, \text{если} \, L\mod 2 = 0.
\end{equation*}
A также разложение $\mathbf X_{B_k} = \mathbf X_k + \mathbf X_{L+2-k} = U_k U_k^H \mathbf X + U_{L+2-k} U_{L+2-k}^H \mathbf X$, где $U^H$ --- это комплексное сопряжение и транспонирование вектора $U$.

Для ясности, $U_k U_k^H + U_{L+2-k} U_{L+2-k}^H$ является оператором проектирования на подпространство, которое порождено синусами и косинусами с частотой $w_k = \frac{k-1}{L}$. Это пространство соответствует компонентам синусоидальной структуры временного ряда, связанных с конкретной частотой, выделяемой методом.

\subsubsection{Группировка} 
Такой же шаг, как и в базовом $\SSA$. Однако группировка будет производиться по частотам.

\subsubsection{Диагональное усреднение}
Такой же шаг, как и в базовом $\SSA$.\newline \newline
\textbf{\large{Нестационарный случай}} \newline \newline 
Для применения данного алгоритма на нестационарных временных рядах, нужно применить процедуру расширения ряда. Как утверждается в статье \cite{bogalo2020}, после расширения, $\CISSA$ можно применить к нестационарному ряду.




\subsection{Свойства}

\subsubsection{Асимптотическая эквивалентность методов}
В статье \cite{bogalo2020} говорится, что асимптотически методы $\SSA$ и $\CISSA$ эквивалентны и в доказательство приводится теорема.

\begin{definition}
	Будем говорить, что методы $M_1$ и $M_2$ асимптотически эквивалентны, если их матрицы вложения $S_1$, $S_2$ асимптотически эквиваленты в смысле $\operatorname*{lim}\limits_{L\rightarrow\infty}\frac{\doublenorm{S_1-S_2}_F}{\sqrt{L}}=0$, где $\doublenorm{\cdot}_F$ --- норма Фробениуса. Тогда $M_1 \sim M_2$, $S_1 \sim S_2$.
\end{definition}

\begin{theorem}
	\label{th:equiv}
	Дана $L \times K$ траекторная матрица $\mathbf{X}$, определенная в \eqref{eq:X}. Пусть $S_B = \mathbf{X} \mathbf{X}^T / K$, $S_T$ --- матрица, определенная в \eqref{eq:tepl_mat}, $S_C$ --- матрица, определенная в \eqref{eq:circ_mat}. Тогда $S_B \sim S_T \sim S_C$.
\end{theorem}

\begin{proof}
	Доказательство в источнике \cite{bogalo2020}.
\end{proof}


Теорема \ref{th:equiv} дает понимание похожих практических результатов при применении разных методов.

\subsubsection{Точная разделимость}
Поскольку данный метод является аналогом разложения Фурье, то в смысле сильной разделимости можно точно разделить ряд, в котором одной из компонентов является $\cos(2\pi w)$ или $\sin(2\pi w)$ с частотой $w$ такой, что $Lw = k \in \mathbb N$, или константа. Поэтому до применения алгоритма необходимо выделить интересующие частоты и, исходя из них, выбирать значение $L$.

\subsubsection{Асимптотическая разделимость}

Асимптотическая разделимость в данном случае будет означать, что при увеличении $L$ разбиение сетки будет увеличиваться, а значит, и частоты в сетке начнут сближаться к истинным частотам периодических компонентов (либо становиться равными им), что будет снижать ошибку вычислений.


\subsubsection{Сравнение CiSSA с разложением Фурье}
Для описания конечных, но достаточно длинных рядов можно использовать разложение Фурье. Пусть $\TS = (x_0, x_1, \dots, x_n, \dots)$ — временной ряд
\begin{definition}
	Разложение
	\begin{equation}
		\label{eq:fourier}
		x_n = c_0 + \sum\limits_{k = 1}^{[N/2]}\left(c_k \cos(2\pi n k / N) + s_k \sin(2\pi n k / N) \right),
	\end{equation}
	где $0 \leq n < N$ и $s_{N/2} = 0 $ для четного N, называется разложением Фурье ряда $\TS$. 
\end{definition}

Алгоритм $\CISSA$ тесно связан с разложением Фурье. А именно, $\CISSA$ можно представить так:
\begin{enumerate}
	\item Вычисляем разложение Фурье для каждого вектора вложения $L$-траекторная матрица $\mathbf{X}$, состоящая из $K = N - L + 1$ векторов. Получается $K$ разложений Фурье по частотам $w_k = \frac{k-1}{L}$, $k = 1:L$;
	\item По получившимся разложениям Фурье усредняем значения для соответствующих $x_i$ и частот $w_k$.
\end{enumerate}

Сравним алгоритмы $\CISSA$ и разложение Фурье по всему ряду.

Данные методы разложения временного ряда должны совпадать, если ряд состоит только из периодических компонент. Например, пусть $\TS = \TS_{\sin} + \TS_{\cos} = \sin{\frac{2\pi}{12}x} + \cos{\frac{2\pi}{8}x}$, $L = 96$, $N = 96 \cdot 2-1$. Сравним результаты по среднеквадратичной ошибке:

\begin{table}[ht]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		Fourier & 1.0e-29 & 4.7e-30 \\ 
		CiSSA & 2.1e-28 & 3.5e-28 \\  
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos}$ методов разложение Фурье и \CISSA} 
	\label{tab:errs_fourier_cissa_sin_cos}
\end{table}

Таблица \ref{tab:errs_fourier_cissa_sin_cos} показывает, что оба разложения сделали правильное (с точностью до вычислений с помощью компьютера) разделение компонентов ряда.

Теперь добавим к этому ряду шум: $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{\mathrm{noise}} = \sin{\frac{2\pi}{12}x} + \cos{\frac{2\pi}{8}x} + \mathrm N(0, 0.1)$, $L = 96$, $N = 96 \cdot 2-1$. Результаты должны ухудшиться.
\begin{table}[ht]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		Fourier & 2.2e-04 & 1.1e-04 \\ 
		CiSSA & 3.7e-04 & 1.1e-04 \\  
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} +\TS_{\mathrm{noise}}$ методов разложение Фурье и \CISSA} 
	\label{tab:errs_fourier_cissa_sin_cos_noised}
\end{table}

По таблице \ref{tab:errs_fourier_cissa_sin_cos_noised} видно, что зашумление ряда дало негативный эффект на ошибку.


Попробуем добавить к ряду непериодическую компоненту. $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e = \sin{\frac{2\pi}{12}x} + \cos{\frac{2\pi}{8}x} + 1 + e^{\frac{x}{100}}$, $L = 96$, $N = 96 \cdot 2-1$. Непериодические компоненты будут отвечать низким частотам. Проблема лишь в том, что с помощью методов разложения Фурье $\CISSA$ невозможно различить между собой две непериодические компоненты, поскольку группировка работает по частотам, элементы разложения неизбежно смешаются между собой. Рассмотрим алгоритм $\CISSA$ без расширения ряда и с ним. Лучше всего себя должен показать алгоритм $\CISSA$ с расширением ряда, поскольку он более гибок к выделению тренда, хуже всего --- разложение Фурье. Будем искать экспоненту и константу по низким частотам, назовем это трендовой составляющей ряда.

\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ & $\TS_{c} + \TS_e$ \\ 
		\hline
		Fourier & 3.1e-03 & 6.8e-03 & 1.1e-01 \\ 
		CiSSA & 1.6e-04 & 4.9e-04 & 5.3e-02 \\  
		CiSSA с расширением ряда & 7.5e-04 & 1.3e-03 & 5.7e-04 \\  
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e$ методов разложение Фурье и \CISSA} 
	\label{tab:errs_fourier_cissa_trend}
\end{table}

Результаты таблицы \ref{tab:errs_fourier_cissa_trend} повторяют вышеизложенные рассуждения, однако периодические компоненты лучше выделились с помощью $\CISSA$ без процедуры расширения ряда.

Теперь добавим шум в предыдущий пример. Результаты всех разложений должны ухудшиться. $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e + \TS_{\mathrm{noise}} = \sin{\frac{2\pi}{12}x} + \cos{\frac{2\pi}{8}x} + 1 + e^{\frac{x}{100}} + \mathrm N(0, 0.1)$, $L = 96$, $N = 96 \cdot 2-1$.

\begin{table}[H]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента & $\TS_{\sin}$ & $\TS_{\cos}$ & $\TS_{c} + \TS_e$ \\ 
		\hline
		Fourier & 3.2e-03 & 7.2e-03 & 1.2e-01 \\ 
		CiSSA & 4.0e-04 & 7.1e-04 & 5.3e-02 \\  
		CiSSA с расширением ряда & 2.3e-03 & 1.7e-03 & 2.2e-03 \\  
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_{\sin} + \TS_{\cos} + \TS_{c} + \TS_e$ методов разложение Фурье и \CISSA} 
	\label{tab:errs_fourier_cissa_trend_noised}
\end{table}

Как видно из таблицы \ref{tab:errs_fourier_cissa_trend_noised}, разделения ухудшились, однако $\CISSA$ без расширения периодические компоненты разделил лучше всех.

\newpage

\section{Сравнение алгоритмов SSA и CiSSA}
%План:
%\begin{enumerate}
%	\item Про базис: SSA --- адаптивный базис (?), CiSSA --- неадаптивный(Фурье разложение).
%	\item Классы точной разделимости: (? их не так много в обоих случаях)
%	\item Классы асимптотической разделимости: SSA --- $x^n$, cos, exp, cos exp; CiSSA разделит cos, остальное смешает между трендом, шумом и теми же косинусами. Подтвердить вычислениями.
%	\item Указать на автоматическое выделение частот в CiSSA, как достоинство, тут же посмотреть на автогруппировку в SSA. Рассмотреть модельные примеры и данные IP.
%	\item Посмотреть, что будет, если частота cos не попадёт в диапазон частот при разложении CiSSA. 
%\end{enumerate}	
Все вычисления, а также код $\CISSA$ можно найти в github репозитории \cite{spbu_cissa_coursework_github}.
\subsection{Собственные пространства}
Каждый алгоритм после группировки порождает построенными матрицами собственные подпространства. В случае базового $\SSA$ алгоритма базис подпространств является адаптивным, то есть зависящим от $\TS, L, N$. Таким образом, $\SSA$ может отличить, например, произведение полиномов, экспонент и косинусов друг от друга.

В случае $\CISSA$ базис зависит только от $L, N$. Если зафиксировать данные параметры, и менять $\TS$, базис никак не поменяется.
	

\subsection{Точная разделимость}
Как удалось выяснить, классов точной разделимости больше в базовом алгоритме $\SSA$, однако в случае разделения $\cos$, условия менее жесткие при использовании $\CISSA$.

Проверим на примерах. 

Возьмем временной ряд, с разложением которого оба алгоритма должны справиться: $\TS = \TS_{C} + \TS_{cos} = 1 + \cos(\frac{2\pi}{12}x)$, $L = 96 \mid 12$, $N = 96 \cdot 2-1$, $K = 96 \mid 12$. Будем считать MSE между настоящими компонентами ряда и вычисленными.
В случае $\SSA$ получилась ошибка при вычислении $C = 1$: $\expnumber{2.1}{}{-30}$, а при вычислении $\cos(\frac{2\pi}{12}x)$: $ \expnumber{4.9}{-30}$. Если применить алгоритм $\CISSA$, получатся ошибки при $C = 1$: $\expnumber{3.6}{-31}$, при $\cos(\frac{2\pi}{12}x)$: $\expnumber{5.2}{-30}$. Эти ошибки можно посчитать за погрешность вычислений на компьютере.

Теперь возьмем временной ряд, при котором $\SSA$ должен отработать хуже $\CISSA$ $\TS = \TS_{C} + \TS_{\cos} = 1 + cos(\frac{2\pi}{12}x)$, $L = 96 \mid 12$, $N = 96 \cdot 2+5$, $K = 102 \nmid 12$. Поскольку $K$ не делится на частоту косинуса, условия точной разделимости в $\SSA$ не выполняются. Будем считать MSE между настоящими компонентами ряда и вычисленными.
В случае $\SSA$ получилась ошибка при вычислении $C = 1$: $\expnumber{9.5}{-5}$, а при вычислении $\cos(\frac{2\pi}{12}x)$: $ \expnumber{9.6}{-5}$. Если применить алгоритм $\CISSA$, получатся ошибки при $C = 1$: $\expnumber{3.2}{-31}$, при $\cos(\frac{2\pi}{12}x)$: $\expnumber{5.1}{-30}$.

Таким образом, с разделением косинуса от константы лучше справился алгоритм $\CISSA$, поскольку в нем требуется меньше условий на параметры алгоритма.

\subsection{Асимптотическая разделимость}
Как было сказано, асимптотически разделимы в методе $\SSA$ полиномы, гармонические функции (косинус, косинус помноженный на экспоненту, экспонента) \cite{golyandina2001analysis}. 
В алгоритме $\CISSA$ при увеличении длины окна $L$ меняется сетка разбиения частот. Из-за этого, даже если не удастся выбрать подходящее $L$, при котором будет точно отделим косинус, но постоянно его увеличивать, в конечном счете получится снизить ошибку выделения нужной компоненты косинуса, если брать соседние частоты с частотой компоненты. Однако в этом случае нужно выбирать диапазон частот, которые стоит объединить.

Непериодические компоненты повлияют на ошибку разложений всего временного ряда, они смешаются и их уже никак не получится отделить методом $\CISSA$. Например, если взять $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin} = 1 + e^{\frac{x}{100}} + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x)$, $N = 96 \cdot 2 - 1, \, L = 96$ , можно получить следующие результаты:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/all.png}
	\caption{Правильное разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$}
	\label{fig:c_e_cos}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/ssa.png}
	\caption{Разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методом $\SSA$}
	\label{fig:c_e_cos_ssa}
\end{figure}

Метод $\SSA$ разделил правильно все компоненты друг от друга.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/cissa.png}
	\caption{Разложение ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методом $\CISSA$}
	\label{fig:c_e_cos_cissa}
\end{figure}

В случае $\CISSA$ получилось так, что экспонента и константа смешались в одну компоненту. Как и в примере сравнении разделения ряда Фурье и $\CISSA$, одни и те же частоты отвечают одновременно и за константу, и за экспоненту.

% latex table generated in R 4.2.2 by xtable 1.8-4 package
% Wed Jun 12 06:57:37 2024
\begin{table}[ht]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента &|& $\TS_e$ & $\TS_c$ & $\TS_c + \TS_e$ &|& $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & |&2.2e-25 & 2.2e-25 & 4.2e-28 &|&3.8e-29 & 1.6e-29 \\ 
		CiSSA &| &none & none & 3.5e-02 & |&1.4e-04 & 1.9e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin}$ методов $\SSA$ и \CISSA} 
	\label{tab:errs}
\end{table}

Таблица \ref{tab:errs} и рисунки \ref{fig:c_e_cos_ssa}, \ref{fig:c_e_cos_cissa} показывают, что метод $\SSA$ справился лучше в сравнении с $\CISSA$, причем как по разделимости, так и по ошибке. В алгоритме $\CISSA$ трендовая составляющая также смешалась с сезонной, поэтому увеличилась ошибка при косинусе.

Или же, если заменить $\TS_{e}$ на $\TS_{e \cdot \cos}$, то есть теперь ряд $\TS = \TS_c + \TS_{e \cdot \cos} + \TS_{\cos} + \TS_{\sin} = 1 + e^{\frac{x}{100}} \cos(\frac{2\pi}{48} x) + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x)$, то получится следующая таблица ошибок:
\begin{table}[ht]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Компонента &|& $\TS_{e \cdot \cos}$ & $\TS_{\sin}$ & $\TS_{\cos}$ \\ 
		\hline
		SSA & |&4.7e-29 & 1.1e-29 & 8.4e-30 \\ 
		CiSSA &| &3.2e-02 & 2.6e-04 & 5.8e-03 \\ 
		\hline
	\end{tabular}
	\caption{MSE разложений ряда $\TS = \TS_c + \TS_{e \cdot \cos} + \TS_{\cos} + \TS_{\sin}$ методов $\SSA$ и \CISSA} 
	\label{tab:errs_exp_cos}
\end{table}

Таким образом, таблица \ref{tab:errs_exp_cos} показывает тот же недостаток у метода $\CISSA$, что и таблица \ref{tab:errs}.

\subsection{Отделение сигнала от шума}
Рассматривая ряд из предыдущего пункта, добавим к нему гауссовский  шум с стандартным отклонением $0.1$: $\TS = \TS_c + \TS_e + \TS_{\cos} + \TS_{\sin} +\TS_{\mathrm{noise}} = 1 + e^{\frac{x}{100}} + \cos(\frac{2\pi}{12} x) + \sin(\frac{2\pi}{24} x) + \mathrm N(0, 0.1)$, $N = 96 \cdot 2 - 1, \, L = 96$. Сделав такой тест $10000$ раз, получим следующий результат по ошибке $\mathrm{MSE}$ между рядом без шума и с шумом:
\begin{table}[ht]
	\centering
	\begin{tabular}{llllllll}
		\hline
		Метод/Статистики &|& min & median & mean & max & sd\\ 
		\hline
		
		SSA &|& 5.8e-04 & 2.0e-03 & 2.1e-03 & 4.9e-03 & 6.2e-04\\ 
		CiSSA &|& 2.5e-02 & 3.4e-02 & 3.4e-02 & 4.9e-02 & 3.7e-03\\ 
		\hline
	\end{tabular}
	\caption{Данные по распределению шума разложений методов $\SSA$ и \CISSA} 
	\label{tab:errs_of_errs}
\end{table}


По таблице \ref{tab:errs_of_errs} можно увидеть что метод $\SSA$ отработал лучше $\CISSA$.



\subsection{Автоматическая группировка и проверка на реальных данных}
Авторы статьи \cite{bogalo2020} выделяют главным преимуществом то, что $\CISSA$ автоматически разделяет компоненты ряда по частотам. Однако есть метод, позволяющий сделать автоматическое объединение частот по периодограмме в методе $\SSA$ \cite{golyandina2023intelligent}. При этом, прежде чем применять его, стоит выполнить процедуру улучшения разделимости. В данной работе будут использоваться методы EOSSA и FOSSA \cite{golyandina2023intelligent}.

Сравним работы этих алгоритмов сначала на модельных примерах, затем на реальных данных. 

Используем те же данные, что и в прошлом примере: $\TS = \TS_c + \TS_e + \TS_{cos} = 1 + e^{\frac{x}{100}} + cos(\frac{2\pi}{12})$, $N = 96 \cdot 2 - 1, \, L = 96$. Применяем алгоритм EOSSA \cite{golyandina2023intelligent} для лучшей разделимости и выбираем в качестве интересующих частот диапазоны $\left(\frac{1}{24}-\varepsilon, \frac{1}{24}+\varepsilon \right), \left(\frac{1}{12}-\varepsilon, \frac{1}{12}+\varepsilon\right), \varepsilon = \frac{1}{97}$. Результаты остаются теми же, как и в таблице \ref{tab:errs} и рисунках \ref{fig:c_e_cos_ssa}, \ref{fig:c_e_cos_cissa}, однако теперь группировка ряда произошла по интересующим частотам.

Теперь рассмотрим реальные данные --- месячные ряды промышленного производства (Industrial Production, IP), index $2010 = 100$, в США. Промышленное производство широко распространено, так как оно указывается в определении рецессии Национальным бюро экономических исследований (NBER), как один из четырех ежемесячных рядов индикаторов, которые необходимо проверять при анализе делового цикла. Выборка охватывает период с января 1970 года по сентябрь 2014 года, поэтому размер выборки составляет $N = 537$. Источником данных является база данных IMF. Эти показатели демонстрируют различные тенденции, сезонность и цикличность (периодические компоненты, которые соответствуют циклам бизнеса). Данные IP также рассматривались в статье \cite{bogalo2020}. Применим как $\CISSA$, так и $\SSA$ с автоматическим определением частот и улучшением разделимости.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_trend.png}
	\caption{Трендовая составляющая данных IP USA}
	\label{fig:IP_trend}
\end{figure}

При применении FOSSA улучшения разделимости алгоритм $\SSA$ выделяет тренд довольно похоже с $\CISSA$. Весь график $\SSA$ тренд EOSSA выглядит более изогнутым при визуальном сравнении с остальными.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_cycle.png}
	\caption{Циклическая составляющая данных IP USA}
	\label{fig:IP_cycle}
\end{figure}

Аналогичная тренду ситуация происходит с цикличностью. В случае EOSSA правый хвост (значения ряда после 2010-ого года) смешался между цикличностью и трендом.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_sesonal.jpg}
	\caption{Сезонная составляющая данных IP USA}
	\label{fig:IP_sesonal}
\end{figure}

Поскольку в базовом $\SSA$ адаптивный базис, сезонность является менее систематичной, разброс значений выше по сравнению с $\CISSA$.

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/IP_residuals.png}
%	\caption{Шум данных IP USA}
%	\label{fig:IP_residuals}
%\end{figure}
Шум же является нормальным во всех случаях.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/trend inseparability example/W-corr.jpg}
	\caption{Матрицы корреляций IP USA}
	\label{fig:W-corr}
\end{figure}
По матрицам корреляции заметно, что при использовании $\SSA$ с улучшением разделимости EOSSA, сильно смешиваются первые по значимости компоненты ряда (они и являются трендовыми и циклическими). 

%\textbf{ОТСЕБЯТИНА}
%Тренд наиболее гибко и лучше отделяется при применении SSA, цикличность отделилась одинаково в обоих случаях, сезонность выглядит куда приличнее при применении CiSSA. Плюс, приходится подбирать параметры разложения в SSA. В CiSSA вообще ничего не надо делать, просто вкинул, отработало замечательно. 


Таким образом, получились довольно похожие результаты в выделении тренда и цикличности при использовании $\SSA$ с FOSSA и $\CISSA$. Несколько иные результаты при $\SSA$ с EOSSA. Сезонная составляющая в силу неадаптивного базиса более строго выглядит для метода $\CISSA$.
 




\newpage

\section{Заключение}
\label{sec:concl}

В данной работе исследован алгоритм $\CISSA$, сравнены методы $\CISSA$ и $\SSA$, и полученные
знания были проверены на реальных и смоделированных примерах с помощью языка R. Оба алгоритма справляются с поставленными задачами, существенным различием является то, что алгоритм $\SSA$ является более гибким: в нем адаптивный базис, есть дополнительные алгоритмы, которые довольно похоже приближают этот алгоритм к $\CISSA$, а также методы для автоматического выбора компонентов по частотам. Метод $\CISSA$ является простым в использовании.


Дальнейшими действиями является рассмотрение других модификаций метода $\SSA$.


\newpage

\bibliographystyle{plain}
\bibliography{ref}


\end{document}

